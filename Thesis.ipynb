{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Initial setup|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display and Copy\n",
    "from IPython.display import display\n",
    "import copy\n",
    "import json\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Numerical and Scientific Computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.spatial.distance import euclidean, cityblock\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.special import expit\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score, make_scorer, precision_score, precision_recall_curve, matthews_corrcoef\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier, LabelSpreading, LabelPropagation\n",
    "\n",
    "# Genetic Algorithms\n",
    "from sklearn_genetic import GASearchCV\n",
    "from sklearn_genetic.callbacks import ConsecutiveStopping, ProgressBar\n",
    "from sklearn_genetic.space import Categorical as GeneticCategorical, Integer\n",
    "\n",
    "# Automated Machine Learning\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "# OS Operations\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Functions and parameters\n",
    "\n",
    "## 1.1. Initial parametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "n_samples = 10000 # size of the full population.\n",
    "n_features = 18 # number of unimodal features.\n",
    "n_multimodal = 2 # number of multimodal features.\n",
    "optim_hp = True # defines if the optimal parameters will be used. If false, the default HP will be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optimization methods\n",
    "\n",
    "### 1.2.1 Scorer: Mean Reciprocal Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(y_true, y_scores, decay_factor=0.5, top_k=100):\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank (MRR) with linear decay of ranks limited to top_k.\n",
    "    \n",
    "    Args:\n",
    "    - y_true (array-like): True binary labels.\n",
    "    - y_scores (array-like): Predicted scores or probabilities.\n",
    "    - decay_factor (float): Linear decay factor for reciprocal ranks. Default is 0.5.\n",
    "    - top_k (int): Top K positions to consider for MRR calculation. Default is 100.\n",
    "    \n",
    "    Returns:\n",
    "    - mrr (float): Mean Reciprocal Rank score limited to top_k.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_scores': y_scores})\n",
    "    df = df.sort_values(by='y_scores', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Limit to top_k positions\n",
    "    df = df.head(top_k)\n",
    "    \n",
    "    ranks = (df['y_true'] == 1).cumsum()  # Cumulative sum of relevant items within top_k\n",
    "    rr = (df['y_true'] == 1) / (1 + decay_factor * ranks)  # Reciprocal ranks with linear decay within top_k\n",
    "    mrr = rr.sum() / len(df)  # Mean of the decayed reciprocal ranks within top_k\n",
    "    \n",
    "    return mrr\n",
    "\n",
    "# Define the custom scorer\n",
    "mrr_scorer = make_scorer(mean_reciprocal_rank, greater_is_better=True, response_method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Genetic algorithm for hyperparameters optimization (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models(random_state, optim_hp=optim_hp):\n",
    "    \"\"\"\n",
    "    Initialize base model and genetic algorithm model with cross-validation.\n",
    "\n",
    "    Args:\n",
    "    - random_state (int): Random state.\n",
    "    - optim_hp (bool): Flag to toggle hyperparameter optimization.\n",
    "\n",
    "    Returns:\n",
    "    - base_model (Pipeline): Base model pipeline.\n",
    "    - model (GASearchCV or Pipeline): Model with or without hyperparameter optimization.\n",
    "    - cv_stratified (StratifiedKFold): Stratified K-Fold cross-validation strategy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the base model using a Random Forest within a pipeline\n",
    "    base_model = make_pipeline(\n",
    "        RandomForestClassifier(\n",
    "            bootstrap=True, \n",
    "            class_weight='balanced_subsample', \n",
    "            random_state=random_state, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Define the Stratified K-Fold cross-validator\n",
    "    cv_stratified = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Parameters grid for the random forest\n",
    "    param_grid = {\n",
    "        'randomforestclassifier__n_estimators': Integer(10, 200, random_state=random_state),\n",
    "        'randomforestclassifier__max_features': GeneticCategorical(np.arange(0.05, 0.625, 0.025).tolist()),\n",
    "        'randomforestclassifier__max_depth': Integer(2, 20, random_state=random_state),\n",
    "        'randomforestclassifier__min_samples_split': Integer(2, 10, random_state=random_state),\n",
    "        'randomforestclassifier__min_samples_leaf': Integer(1, 10, random_state=random_state)\n",
    "    }\n",
    "\n",
    "    # Initialize GASearchCV for hyperparameter optimization\n",
    "    model = GASearchCV(\n",
    "        estimator=base_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=mrr_scorer,  # Custom scorer defined earlier\n",
    "        population_size=50,\n",
    "        generations=40,\n",
    "        mutation_probability=0.1,\n",
    "        n_jobs=-1,\n",
    "        verbose=True,\n",
    "        cv=cv_stratified\n",
    "    )\n",
    "\n",
    "    # Optionally disable hyperparameter optimization\n",
    "    if not optim_hp:\n",
    "        model = base_model\n",
    "\n",
    "    return base_model, model, cv_stratified\n",
    "\n",
    "# Define stopping rules and visualisation options\n",
    "callback_pr = ProgressBar() # shows a progress bar\n",
    "callback_st = ConsecutiveStopping(generations=1, metric='fitness') # stops if fitness stays the same\n",
    "callback = [callback_pr, callback_st]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Genetic algorithm and grid search for semi-supervised methods\n",
    "\n",
    "Necessary to work with sample with unlabelled class (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_par(X, y, random_state, cv_stratified, method, base_model=''):\n",
    "    \"\"\"\n",
    "    Optimizes parameters for different semi-supervised learning methods.\n",
    "\n",
    "    Args:\n",
    "    - X: Feature matrix.\n",
    "    - y: Target vector.\n",
    "    - random_state (int): Random state for reproducibility.\n",
    "    - cv_stratified (StratifiedKFold): Cross-validation strategy.\n",
    "    - method (str): Method for optimization ('label_spreading', 'self-learning', 'lab_propag').\n",
    "    - base_model (str): Base model for self-learning. Default is ''.\n",
    "\n",
    "    Returns:\n",
    "    - best_parameters (dict): Dictionary with best parameters found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Label Spreading Method\n",
    "    if method == 'label_spreading':\n",
    "        # Create the TPOTClassifier with custom configuration\n",
    "        tpot = TPOTClassifier(\n",
    "            generations=20,  # Number of generations to run the algorithm\n",
    "            population_size=20,  # Size of the population\n",
    "            verbosity=2,  # Verbosity level\n",
    "            config_dict={\n",
    "                'sklearn.semi_supervised.LabelSpreading': {\n",
    "                    'gamma': [0.1, 0.5],\n",
    "                    'alpha': [0.1, 0.2, 0.5]\n",
    "                }\n",
    "            },\n",
    "            scoring=mrr_scorer,  # Custom scorer\n",
    "            early_stop=1, # stops if no best fitness if found in 2 consecutive generations\n",
    "            cv=cv_stratified,  # Number of cross-validation folds\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Fit the TPOT model on the data\n",
    "        tpot.fit(X, y)\n",
    "        \n",
    "        # Extract the best parameters\n",
    "        best_parameters = tpot.fitted_pipeline_.steps[-1][1].get_params()\n",
    "\n",
    "    # Self-Learning Method\n",
    "    elif method == 'self-learning':\n",
    "        # Define the self-training classifier\n",
    "        self_training = SelfTrainingClassifier(base_estimator=base_model)\n",
    "        \n",
    "        # Define the parameter grid\n",
    "        param_grid = [\n",
    "            {'criterion': ['threshold'], 'threshold': list(np.arange(0.1, 1.0, 0.1))},\n",
    "            {'criterion': ['k_best'], 'k_best': [3, 5, 10]}\n",
    "        ]\n",
    "\n",
    "        # Create the GridSearchCV object\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=self_training,\n",
    "            param_grid=param_grid,\n",
    "            scoring=mrr_scorer,  # Custom scorer\n",
    "            cv=cv_stratified,  # Cross-validation strategy\n",
    "            verbose=2, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Fit the GridSearchCV object to the data\n",
    "        grid_search.fit(X, y)\n",
    "        \n",
    "        # Extract the best parameters\n",
    "        best_parameters = grid_search.best_params_\n",
    "\n",
    "    # Label Propagation Method\n",
    "    elif method == 'lab_propag':\n",
    "        # Define the label propagation classifier\n",
    "        lab_propag_model = LabelPropagation(kernel='rbf', max_iter=50, n_jobs=-1, tol=0.0001)\n",
    "        \n",
    "        # Define the parameter grid\n",
    "        param_grid = {\n",
    "            'gamma': [0.1, 0.2, 0.5, 1]\n",
    "        }\n",
    "\n",
    "        # Create the GridSearchCV object\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=lab_propag_model,\n",
    "            param_grid=param_grid,\n",
    "            scoring=mrr_scorer,  # Custom scorer\n",
    "            cv=cv_stratified,  # Cross-validation strategy\n",
    "            verbose=2, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Fit the GridSearchCV object to the data\n",
    "        grid_search.fit(X, y)\n",
    "        \n",
    "        # Extract the best parameters\n",
    "        best_parameters = grid_search.best_params_\n",
    "\n",
    "    # Print the best parameters found\n",
    "    print(\"Best parameters found: \", best_parameters)\n",
    "\n",
    "    return best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Performance measures\n",
    "\n",
    "### 1.3.1. Precision on the top 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_precision_scorer(y_true, y_proba, k=100):\n",
    "    \"\"\"\n",
    "    Calculate the precision on the top K instances with the highest predicted probability for class 1.\n",
    "\n",
    "    In:\n",
    "    - y_proba: The predicted probabilities for class 1.\n",
    "    - k: The number of top instances to consider (default is 100).\n",
    "\n",
    "    Out:\n",
    "    - precision: The precision score on the top K instances.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the indices of the top K instances with the highest probability for class 1\n",
    "    top_k_indices = np.argsort(y_proba)[-k:]\n",
    "\n",
    "    # Get the true labels and predicted labels for the top K instances\n",
    "    y_true_top_k = y_true[top_k_indices]\n",
    "\n",
    "    # Calculate precision score on the top K instances\n",
    "    precision = (sum(y_true_top_k == 1)) / k\n",
    "\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Bias in the sample\n",
    "\n",
    "Evaluates how distant the centroids are between the sample and the population, for class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_bias_in_sample(X_test, X_pred, y_test, y_pred, verbose=False):\n",
    "    \"\"\"\n",
    "    Measure the bias in predictions by calculating the distance between centroids of \n",
    "    true class 1 and predicted class 1 instances using Euclidean and Manhattan distances.\n",
    "\n",
    "    Parameters:\n",
    "    - X_test: DataFrame containing true instances (features).\n",
    "    - X_pred: DataFrame containing predicted instances (features).\n",
    "    - y_test: Series or array containing the true labels (0s and 1s).\n",
    "    - y_pred: Series or array containing the predicted labels (0s and 1s).\n",
    "    - verbose: Boolean flag for verbose output (default is False).\n",
    "\n",
    "    Returns:\n",
    "    - manhattan_distance: The Manhattan distance between centroids.\n",
    "    - euclidean_distance: The Euclidean distance between centroids.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure y_test and y_pred are pandas Series\n",
    "    y_test = pd.Series(y_test)\n",
    "    y_pred = pd.Series(y_pred)\n",
    "\n",
    "    # Filter X_test and X_pred where y_test is 1 and where y_pred is 1\n",
    "    X_true = X_test[y_test == 1]\n",
    "    X_pred_true = X_pred[y_pred == 1]\n",
    "\n",
    "    # Initialize scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit scaler on X_true\n",
    "    scaler.fit(X_true)\n",
    "\n",
    "    # Scale X_true and X_pred_true\n",
    "    X_true_scaled = pd.DataFrame(scaler.transform(X_true))\n",
    "    X_pred_true_scaled = pd.DataFrame(scaler.transform(X_pred_true))\n",
    "\n",
    "    # Calculate centroids for all true class 1 instances\n",
    "    centroid_true_all = X_true_scaled.mean().values\n",
    "\n",
    "    # Calculate centroids for the predicted class 1 instances\n",
    "    centroid_pred_true = X_pred_true_scaled.mean().values\n",
    "\n",
    "    # Calculate Euclidean distance between the centroids\n",
    "    euclidean_distance = euclidean(centroid_true_all, centroid_pred_true)\n",
    "\n",
    "    # Calculate Manhattan distance between the centroids\n",
    "    manhattan_distance = cityblock(centroid_true_all, centroid_pred_true)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Euclidean distance between centroids: {euclidean_distance}\")\n",
    "        print(f\"Manhattan distance between centroids: {manhattan_distance}\")\n",
    "\n",
    "    return manhattan_distance, euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. Prediction bias\n",
    "\n",
    "Calculates how distant the centroids are between the top-100 predicted class 1 and the real class 1 cases in the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_bias(X, y_test, y_pred_prob, verbose=False):\n",
    "    \"\"\"\n",
    "    Measure the bias in predictions by calculating the distance between centroids of \n",
    "    true class 1 and top-100 predicted class 1 instances using Euclidean and Manhattan distances.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame containing the full list of features.\n",
    "    - y_test: Series or array containing the true labels (0s and 1s).\n",
    "    - y_pred_prob: Series or array containing the predicted probabilities for class 1.\n",
    "    - verbose: Boolean flag for verbose output (default is False).\n",
    "\n",
    "    Returns:\n",
    "    - manhattan_distance: The Manhattan distance between centroids.\n",
    "    - euclidean_distance: The Euclidean distance between centroids.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure y_test and y_pred_prob are pandas Series\n",
    "    y_test = pd.Series(y_test)\n",
    "    y_pred_prob = pd.Series(y_pred_prob)\n",
    "\n",
    "    # Identify the indices of the 100 highest predicted probabilities\n",
    "    top_100_indices = np.argsort(y_pred_prob)[-100:]\n",
    "    \n",
    "    # Create y_pred with the top 100 indices set to 1\n",
    "    y_pred = pd.Series(np.zeros_like(y_pred_prob))\n",
    "    y_pred.iloc[top_100_indices] = 1\n",
    "    \n",
    "    # Convert X to DataFrame if it's a NumPy array\n",
    "    if isinstance(X, np.ndarray):\n",
    "        feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "    X = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # Filter scaled X where y_test is 1 and where y_pred is 1\n",
    "    X_pred_top_100 = X_scaled[y_pred == 1]\n",
    "    X_true_all = X_scaled[y_test == 1]\n",
    "\n",
    "    # Handle NaN values after filtering\n",
    "    X_pred_top_100 = X_pred_top_100.dropna()\n",
    "    X_true_all = X_true_all.dropna()\n",
    "\n",
    "    # Calculate centroids for all true class 1 instances\n",
    "    centroid_true_all = X_true_all.mean().values\n",
    "\n",
    "    # Calculate centroids for the top 100 predicted class 1 instances\n",
    "    centroid_pred_top_100 = X_pred_top_100.mean().values\n",
    "\n",
    "    # Calculate Euclidean distance between the centroids\n",
    "    euclidean_distance = euclidean(centroid_true_all, centroid_pred_top_100)\n",
    "\n",
    "    # Calculate Manhattan distance between the centroids\n",
    "    manhattan_distance = cityblock(centroid_true_all, centroid_pred_top_100)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Euclidean distance between centroids: {euclidean_distance}\")\n",
    "        print(f\"Manhattan distance between centroids: {manhattan_distance}\")\n",
    "\n",
    "    return manhattan_distance, euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. Generate performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance(X_train, y_train, X_test, y_test, model_i, std_model, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns several performance measures of a model trained in X_train and tested in X_test.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: DataFrame or array-like, training features.\n",
    "    - y_train: Series or array-like, training labels.\n",
    "    - X_test: DataFrame or array-like, test features.\n",
    "    - y_test: Series or array-like, test labels.\n",
    "    - model_i: Initialized machine learning model.\n",
    "    - std_model: Standardized model for hyperparameter optimization.\n",
    "    - verbose: Boolean flag for verbose output (default is False).\n",
    "\n",
    "    Returns:\n",
    "    - result: Dictionary containing performance metrics.\n",
    "    - best_hp: Dictionary of best hyperparameters (if hyperparameter optimization is performed).\n",
    "    \"\"\"\n",
    "\n",
    "    best_hp = []\n",
    "\n",
    "    # Hyperparameter optimization\n",
    "    if optim_hp and (type(model_i) == type(std_model)):\n",
    "        model_i.fit(X_train, y_train, callbacks=callback)\n",
    "        best_hp = model_i.best_params_\n",
    "        best_hp = {key.replace('randomforestclassifier__', ''): value for key, value in best_hp.items()}\n",
    "        if verbose:\n",
    "            print(\"Best Hyperparameters for the Random Forest:\", best_hp)\n",
    "    else:\n",
    "        model_i.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities and labels on the test set\n",
    "    y_pred_prob = model_i.predict_proba(X_test)[:, 1]\n",
    "    y_pred = model_i.predict(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    prec_score = precision_score(y_test, y_pred, zero_division=0)\n",
    "    auprc_score = average_precision_score(y_test, y_pred_prob)\n",
    "    top_50 = top_k_precision_scorer(y_test, y_pred_prob, k=50)\n",
    "    top_100 = top_k_precision_scorer(y_test, y_pred_prob, k=100)\n",
    "    \n",
    "    # Calculate bias on the feature used for selection bias\n",
    "    bias = measure_bias(X_test, y_test, y_pred_prob)[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Precision score:\", prec_score)\n",
    "        print(\"AUPRC score:\", auprc_score)\n",
    "        print(\"Precision on top 50:\", top_50)\n",
    "        print(\"Precision on top 100:\", top_100)\n",
    "        print(\"Prediction bias:\", bias)\n",
    "\n",
    "    # Construct result dictionary\n",
    "    result = {\n",
    "        'pr': prec_score,\n",
    "        'AUPRC': auprc_score,\n",
    "        'p.50': top_50,\n",
    "        'p.100': top_100,\n",
    "        'Bias effect': bias\n",
    "    }\n",
    "\n",
    "    return result, best_hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simulations\n",
    "\n",
    "## 2.1. Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pop(random_state, size=n_samples, feat=n_features, multimodal = n_multimodal, ratio=0.5, verbose = True):\n",
    "    \"\"\"\n",
    "    Creates simulated populations.\n",
    "\n",
    "    In:\n",
    "    - Size, number of features, ratio of one of the classes. \n",
    "    \n",
    "    Out: \n",
    "    - A current population, used to extract (biased) samples;\n",
    "    - A future and similar population, used as test set; the feature with the highest correlation with the target.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Define simulation parameters\n",
    "    se = 2  # Standard deviation of error term\n",
    "    samples = size * 2\n",
    "\n",
    "    # Generate predictor variables (features) X\n",
    "    # Generate random means and standard deviations for features\n",
    "    means = np.random.rand(feat)  # Random means between 0 and 1 (inclusive)\n",
    "    #means = np.zeros(feat)  # To make the data less complex\n",
    "    stds = np.random.rand(feat) * 0.3 + 0.8  # Standard deviations between 0.8 and 1.1\n",
    "    #stds = np.ones(feat) # To make the data less complex\n",
    "    # All features follow a normal distribution\n",
    "    X = np.random.normal(loc=means, scale=stds, size=(samples, feat))\n",
    "\n",
    "    for _ in range(multimodal):\n",
    "        # Generates a random number 'modes' from 2 to 4 for the number of modes\n",
    "        modes = np.random.randint(2, 4)\n",
    "        # Creates random means\n",
    "        mode_means = np.random.rand(modes)\n",
    "        \n",
    "        # Samples from all normal distributions to get a single multimodal feature\n",
    "        mode_samples = np.hstack([np.random.normal(loc=mean, scale=1, size=(samples, 1)) for mean in mode_means])\n",
    "        multimodal_feature = np.random.choice(mode_samples.flatten(), size=samples).reshape(samples, 1)\n",
    "        \n",
    "        X = np.hstack((X, multimodal_feature))\n",
    "\n",
    "    # Define random coefficients (excluding intercept)\n",
    "    coefficients = np.random.rand(feat + multimodal)\n",
    "\n",
    "    # Define error term\n",
    "    error_t = np.random.normal(0, se, samples)\n",
    "\n",
    "    # Function to find the optimal intercept\n",
    "    def objective(intercept):\n",
    "        eta = np.dot(X, coefficients) + intercept + error_t\n",
    "        probabilities = expit(eta)\n",
    "        y = (probabilities > 0.5).astype(int)\n",
    "        actual_ratio = y.mean()\n",
    "        return (actual_ratio - ratio) ** 2\n",
    "\n",
    "    # Find the intercept that minimizes the difference from the desired ratio\n",
    "    result = minimize_scalar(objective)\n",
    "    intercept = result.x\n",
    "\n",
    "    # Generate linear predictor (eta) with the optimal intercept\n",
    "    eta = np.dot(X, coefficients) + intercept + error_t\n",
    "\n",
    "    # Apply link function (logit) to obtain probabilities\n",
    "    probabilities = expit(eta)\n",
    "\n",
    "    # Generate class labels directly from the probabilities\n",
    "    y = (probabilities > 0.5).astype(int)\n",
    "\n",
    "    # Divides the data into two groups: current population and future population\n",
    "    # The `train_size` parameter specifies the size of the current population\n",
    "    X_cur_pop, X_fut_pop, y_cur_pop, y_fut_pop = train_test_split(\n",
    "        X, y,\n",
    "        train_size=size,  # Size of current population\n",
    "        stratify=y,  # Ensure stratified sampling to preserve class ratios\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        # Ensure the class imbalance is as desired\n",
    "        actual_ratio = y.mean()\n",
    "        print(\"Desired class ratio:\", ratio)\n",
    "        print(\"Actual class ratio:\", actual_ratio)\n",
    "        # Print statements to check internal states\n",
    "        #print(\"Max correlation:\", max_abs_corr_value)\n",
    "\n",
    "    # Return results\n",
    "    return (X_cur_pop, X_fut_pop, y_cur_pop, y_fut_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Plots and summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_population(X, y):\n",
    "    \"\"\"\n",
    "    Plots the population, differentiating by colour according to the class.\n",
    "    Dimension reduction (PCA) is applied the allow the presentation in a two-dimentional plot.\n",
    "    Two principal components are sufficient because there are only two informative features in the similated population.\n",
    "\n",
    "    In:\n",
    "    - X, a set of features.\n",
    "    - y, the correspondent class sabels.\n",
    "    \n",
    "    Out:\n",
    "    - A plot of the population, printed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Instantiate a PCA object and fit it on X_cur_pop to get 2 principal components (2 informative features)\n",
    "    y_copy = np.copy(y)\n",
    "    X_copy = np.copy(X)\n",
    "    \n",
    "    if X.shape[1] > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        X_cur_pop_pca = pca.fit_transform(X_copy)\n",
    "\n",
    "        # Order X_cur_pop_pca and y_copy by y_copy\n",
    "        sorted_indices = np.argsort(y_copy)\n",
    "        X_cur_pop_pca = X_cur_pop_pca[sorted_indices]\n",
    "        y_copy = y_copy[sorted_indices]\n",
    "\n",
    "        # Create a scatter plot of the 2 principal components\n",
    "        # The first principal component (PC1) is in the first column of X_cur_pop_pca\n",
    "        # The second principal component (PC2) is in the second column of X_cur_pop_pca\n",
    "\n",
    "        # Use seaborn for a nicer plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(\n",
    "            x=X_cur_pop_pca[:, 0],  # PC1\n",
    "            y=X_cur_pop_pca[:, 1],  # PC2\n",
    "            hue=y_copy,  # Color points based on y\n",
    "            palette={-1.: 'grey', 0.: 'blue', 1.: 'red'}, # -1 means unlabeled.\n",
    "            s=1\n",
    "        )\n",
    "        # Add plot labels\n",
    "        plt.xlabel('PC1')\n",
    "        plt.ylabel('PC2')\n",
    "        plt.title('PCA of X colored by target feature')\n",
    "    else:\n",
    "        # Order X_cur_pop_pca and y_copy by y_copy\n",
    "        sorted_indices = np.argsort(y_copy)\n",
    "        X_sorted = X_copy[sorted_indices]\n",
    "        y_sorted = y_copy[sorted_indices]\n",
    "\n",
    "        # Use seaborn for a nicer plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(\n",
    "            x=X_sorted[:, 0],  # Feature 1\n",
    "            y=X_sorted[:, 1],  # Feature 2\n",
    "            hue=y_sorted,  # Color points based on y\n",
    "            palette={-1.: 'grey', 0.: 'blue', 1.: 'red'}, # -1 means unlabeled.\n",
    "            s=1\n",
    "        )\n",
    "        # Add plot labels\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_pop(X, y, full_X, plot_pop = False):\n",
    "    \"\"\"\n",
    "    Prints a summary of a population or sample.\n",
    "\n",
    "    In:\n",
    "    - X (ndarray or DataFrame): Features.\n",
    "    - y (ndarray or Series): Target feature.\n",
    "    - full_X (ndarray or DataFrame): The complete population for context.\n",
    "\n",
    "    Out:\n",
    "    - Plots the population in two dimensions\n",
    "    - Prints summary information about the population or sample.\n",
    "    \"\"\"\n",
    "    # NUMERIC SUMMARY:\n",
    "\n",
    "    # Calculate the number of features and observations\n",
    "    num_features = X.shape[1]\n",
    "    num_observations = X.shape[0]\n",
    "    \n",
    "    # Calculate the percentage of the population represented by the sample\n",
    "    full_population_size = full_X.shape[0]\n",
    "    population_percentage = (num_observations / full_population_size) * 100\n",
    "    \n",
    "    # Calculate the number and percentage of infractors and compliers\n",
    "    num_infractors = sum(y == 1)\n",
    "    num_compliers = sum(y == 0)\n",
    "    infractor_percentage = (num_infractors / num_observations) * 100\n",
    "    complier_percentage = (num_compliers / num_observations) * 100\n",
    "    \n",
    "    # Print summary information\n",
    "    print(f\"Number of features: {num_features}\")\n",
    "    print(f\"Number of labeled observations: {num_observations} ({population_percentage:.2f}% of {full_population_size})\")\n",
    "    print(f\"Violators: {num_infractors} ({infractor_percentage:.2f}%)\")\n",
    "    print(f\"Compliers: {num_compliers} ({complier_percentage:.2f}%)\")\n",
    "\n",
    "    # PLOTS:\n",
    "    if plot_pop:\n",
    "        plot_population(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Sample\n",
    "\n",
    "Simulates the labelling of a sample of cases, with or without labelling bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_cases(X, # full population of features\n",
    "                y, # full population of correspondent targets\n",
    "                random_state,\n",
    "                sample_size=0.05, # percentage of the population in the sample\n",
    "                sample_bias=0.5, # percentage of cases of class 1\n",
    "                prop_random=0.2,# proportion of the sample composed with random sampling\n",
    "                verbose=False,\n",
    "                ):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function labels a sample of the data based on specified biases and random sampling.\n",
    "\n",
    "    Parameters:\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The input samples.\n",
    "    y : array-like, shape (n_samples,)\n",
    "        The target values.\n",
    "    random_state : int\n",
    "        The random seed for reproducibility.\n",
    "    sample_size : float, optional (default=0.05)\n",
    "        The percentage of the population in the sample.\n",
    "    sample_bias : float, optional (default=0.5)\n",
    "        The percentage of cases of class 1 in the sample.\n",
    "    prop_random : float, optional (default=0.2)\n",
    "        The proportion of the sample composed of random sampling.\n",
    "    verbose : bool, optional (default=False)\n",
    "        Whether to print detailed information.\n",
    "\n",
    "    Returns:\n",
    "    X_lab : array-like, shape (n_samples_lab, n_features)\n",
    "        The labeled sample.\n",
    "    X_unl : array-like, shape (n_samples_unl, n_features)\n",
    "        The unlabeled sample.\n",
    "    y_lab : array-like, shape (n_samples_lab,)\n",
    "        The target values for the labeled sample.\n",
    "    y_unl : array-like, shape (n_samples_unl,)\n",
    "        The target values for the unlabeled sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    if sample_bias == 0:\n",
    "        # No bias, use stratified sampling for labeled and unlabeled split\n",
    "        X_lab, X_unl, y_lab, y_unl = train_test_split(\n",
    "            X, y,\n",
    "            train_size=sample_size,\n",
    "            random_state=random_state,\n",
    "            stratify=y  # Ensure stratified sampling to preserve class ratios\n",
    "        )\n",
    "        return X_lab, X_unl, y_lab, y_unl\n",
    "\n",
    "    else:\n",
    "        # With bias, create labeled and unlabeled samples based on selector feature\n",
    "        full_pop = pd.DataFrame(X.copy()).reset_index(drop=True)\n",
    "        full_pop['target'] = y\n",
    "        full_pop['label'] = -1\n",
    "\n",
    "        n = int(len(X) * sample_size)  # Calculate the number of cases in the sample\n",
    "        n_1 = int(n * sample_bias)    # Calculate the desired number of class 1 cases\n",
    "        n_0 = n - n_1                # Calculate the desired number of class 0 cases\n",
    "        if verbose:\n",
    "            print(f\"Desired samples per class:\")\n",
    "            print(f\"     Class 1: {n_1}\")\n",
    "            print(f\"     Class 0: {n_0}\")\n",
    "\n",
    "        if prop_random > 0:\n",
    "            # Select a fully random sample from the population and assigns labels\n",
    "            random_sample_indices = full_pop.sample(n=int(prop_random * n), random_state=random_state).index\n",
    "            full_pop.loc[random_sample_indices, 'label'] = full_pop.loc[random_sample_indices, 'target']\n",
    "            # Compute how many class 1 and class 0 cases we still need to label\n",
    "            n_1 = max(n_1 - sum(full_pop['label'] == 1), 0)\n",
    "            n_0 = max(n_0 - sum(full_pop['label'] == 0), 0)\n",
    "            if verbose:\n",
    "                total_random_cases = len(random_sample_indices)\n",
    "                percent_random_cases = (total_random_cases / n) * 100\n",
    "                print(f\"Random cases included: {total_random_cases} ({percent_random_cases:.2f}%)\")\n",
    "                print(f\"     Class 1: {sum(full_pop.loc[random_sample_indices, 'target'] == 1)}\")\n",
    "                print(f\"     Class 0: {sum(full_pop.loc[random_sample_indices, 'target'] == 0)}\")\n",
    "                print(f\"Cases still to add: {n_1 + n_0}\")\n",
    "                print(f\"     Class 1: {n_1}\")\n",
    "                print(f\"     Class 0: {n_0}\")\n",
    "\n",
    "        # Create 'selector', a feature that explains part of the selection bias\n",
    "        # Calculate the correlation of each feature with the target\n",
    "        correlations = full_pop.corr()['target'].drop('target')\n",
    "        # Identify the two features with the highest absolute correlation with the target\n",
    "        top_features = correlations.abs().nlargest(2).index\n",
    "        # Center the top features by subtracting their mean\n",
    "        centered_features = full_pop[top_features].apply(lambda x: x - x.mean())\n",
    "        # Multiply each centered feature by its correlation with the target\n",
    "        adjusted_features = centered_features * correlations[top_features].values\n",
    "        # Compute the average of these two adjusted features to create the selector\n",
    "        full_pop['selector'] = adjusted_features.mean(axis=1)\n",
    "        full_pop = full_pop.sort_values(by='selector', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        # Completes the sample with necessary 1s\n",
    "        potential_1_indices = full_pop[(full_pop['target'] == 1) & (full_pop['label'] == -1)].head(int(1.5 * n_1)).index\n",
    "        selected_1_indices = np.random.choice(potential_1_indices, size=n_1, replace=False)\n",
    "        full_pop.loc[selected_1_indices, 'label'] = 1\n",
    "        # Completes the sample with necessary 0s\n",
    "        potential_0_indices = full_pop[(full_pop['target'] == 0) & (full_pop['label'] == -1)].head(int(1.5 * n_0)).index\n",
    "        selected_0_indices = np.random.choice(potential_0_indices, size=n_0, replace=False)\n",
    "        full_pop.loc[selected_0_indices, 'label'] = 0\n",
    "\n",
    "        # Extract the labeled instances\n",
    "        labeled = full_pop[full_pop['label'] != -1].sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "        X_lab = labeled.drop(columns=['target', 'selector', 'label'])\n",
    "        y_lab = labeled['target']\n",
    "\n",
    "        # Extract the unlabeled instances\n",
    "        unlabeled = full_pop[full_pop['label'] == -1].sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "        X_unl = unlabeled.drop(columns=['target', 'selector', 'label'])\n",
    "        y_unl = unlabeled['target']\n",
    "\n",
    "    if verbose:\n",
    "        print(\"LABELED SAMPLE CREATED:\")\n",
    "        print(\"Labelling bias:\", sample_bias)\n",
    "        describe_pop(X_lab, y_lab, X)\n",
    "        # PLOTS:\n",
    "        full_X = np.vstack((X_lab, X_unl))\n",
    "        full_y = np.concatenate([y_lab, np.full((X_unl.shape[0],), -1)])\n",
    "        plot_population(full_X, full_y)\n",
    "\n",
    "    return X_lab, X_unl, y_lab, y_unl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Methods\n",
    "\n",
    "## 3.1. Veenman Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_Veenman(X_lab, X_unl, y_lab, y_unl, seed=42):\n",
    "    \"\"\"\n",
    "    Creates a unified DataFrame with labeled and unlabeled cases, adding columns to identify if it's labeled \n",
    "    and the pseudoclass based on the Veenman method.\n",
    "\n",
    "    Parameters:\n",
    "    - X_lab: numpy array of labeled features\n",
    "    - X_unl: numpy array of unlabeled features\n",
    "    - y_lab: numpy array of labeled targets\n",
    "    - y_unl: numpy array of unlabeled targets\n",
    "    - seed: random seed for reproducibility (default=42)\n",
    "\n",
    "    Returns:\n",
    "    - A single DataFrame with all cases, including 'label', 'labeled', and 'pseudo' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create DataFrames from the numpy arrays\n",
    "    df_unlabeled = pd.DataFrame(X_unl)\n",
    "    df_unlabeled[\"label\"] = y_unl\n",
    "    df_labeled = pd.DataFrame(X_lab)\n",
    "    df_labeled[\"label\"] = y_lab\n",
    "\n",
    "    # Add a 'labeled' column: 0 for unlabeled data, 1 for labeled data\n",
    "    df_unlabeled['labeled'] = 0\n",
    "    df_labeled['labeled'] = 1\n",
    "\n",
    "    # Concatenate all data into one df\n",
    "    df_combined = pd.concat([df_unlabeled, df_labeled], ignore_index=True)\n",
    "\n",
    "    # Initialize the \"pseudo\" column with default value 999\n",
    "    df_combined['pseudo'] = 999\n",
    "    # Update \"pseudo\" column based on the conditions:\n",
    "    df_combined.loc[(df_combined['labeled'] == 0), 'pseudo'] = 0\n",
    "    df_combined.loc[(df_combined['labeled'] == 1) & (df_combined['label'] == 0), 'pseudo'] = -1\n",
    "    df_combined.loc[(df_combined['labeled'] == 1) & (df_combined['label'] == 1), 'pseudo'] = 1\n",
    "    \n",
    "    # Shuffle rows\n",
    "    df_combined = df_combined.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Veenman(X_lab, X_unl, y_lab, y_unl, verbose=False):\n",
    "    \"\"\"\n",
    "    Creates a train set composed of all positive labeled instances, plus all unlabeled instances, which are considered as 0.\n",
    "\n",
    "    Parameters:\n",
    "    - X_lab: numpy array of labeled features\n",
    "    - X_unl: numpy array of unlabeled features\n",
    "    - y_lab: numpy array of labeled targets\n",
    "    - y_unl: numpy array of unlabeled targets\n",
    "    - verbose: bool, if True prints information and plots (default=False)\n",
    "\n",
    "    Returns:\n",
    "    - X_V_lab: DataFrame of pseudo-labeled features\n",
    "    - y_V_lab: Series of pseudo-labeled targets\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the combined DataFrame with pseudo labels\n",
    "    df_combined = init_Veenman(X_lab, X_unl, y_lab, y_unl)\n",
    "\n",
    "    # Separate the labeled and unlabeled instances based on the pseudo label\n",
    "    X_V_lab = df_combined.loc[df_combined['pseudo'] >= 0, df_combined.columns != 'pseudo'].drop(columns=['labeled', 'label']).reset_index(drop=True)\n",
    "    y_V_lab = df_combined.loc[df_combined['pseudo'] >= 0, 'pseudo'].reset_index(drop=True)\n",
    "    X_V_unl = df_combined.loc[df_combined['pseudo'] == -1, df_combined.columns != 'pseudo'].drop(columns=['labeled', 'label']).reset_index(drop=True)\n",
    "    y_V_unl = df_combined.loc[df_combined['pseudo'] == -1, 'pseudo'].reset_index(drop=True)\n",
    "\n",
    "    # Reset indices\n",
    "    X_V_lab.reset_index(drop=True, inplace=True)\n",
    "    y_V_lab.reset_index(drop=True, inplace=True)\n",
    "    X_V_unl.reset_index(drop=True, inplace=True)\n",
    "    y_V_unl.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if verbose:\n",
    "        full_X = np.vstack((X_V_lab, X_V_unl))\n",
    "        full_y = np.concatenate([y_V_lab, y_V_unl])\n",
    "        print(\"VEENMAN SAMPLE CREATED:\")\n",
    "        describe_pop(X_V_lab, y_V_lab, full_X)  # Ensure describe_pop function is defined\n",
    "        # PLOTS:\n",
    "        plot_population(full_X, full_y)  # Ensure plot_population function is defined\n",
    "\n",
    "    return X_V_lab, y_V_lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Semi-supervised method\n",
    "\n",
    "First, a \"Veenman\" sample is selected. Then, the unlabeled sample is divided into folds. For each fold, a model is trained on the Veenman sample minus the instances of the fold. Predicted class 1 for the fold are assigned as pseudo-labels.\n",
    "\n",
    "The final sample comprises the original \"Veenman\" sample, plus the predicted pseudo-labels 1 for unlabeled instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_folder(population, folds, random_state):\n",
    "    \"\"\"\n",
    "    Assigns folder numbers for each unlabeled instance using K-Fold cross-validation.\n",
    "\n",
    "    Args:\n",
    "    - population (pd.DataFrame): The full population/sample.\n",
    "    - folds (int): The number of folds.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The population/sample with a column \"fold\" with a fold number per instance.\n",
    "    \"\"\"\n",
    "    # Initialize the \"fold\" column with -1\n",
    "    population['fold'] = -1\n",
    "\n",
    "    mask_unlabeled = population['labeled'] == 0\n",
    "    unlabeled_data = population[mask_unlabeled]\n",
    "\n",
    "    # Create a KFold object\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Assign folds to unlabeled data using KFold\n",
    "    for fold, (_, test_index) in enumerate(kf.split(unlabeled_data)):\n",
    "        unlabeled_data.iloc[test_index, unlabeled_data.columns.get_loc('fold')] = fold\n",
    "\n",
    "    # Update the population with the fold assignments for the unlabeled data\n",
    "    population.loc[mask_unlabeled, 'fold'] = unlabeled_data['fold']\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised(X_lab, X_unl, y_lab, y_unl, model_i, base_model,\n",
    "                    random_state,\n",
    "                    folds = 5, threshold = 0.5,\n",
    "                    population_proportion = 9999,\n",
    "                    sample_bias = 99999,\n",
    "                    verbose = False):\n",
    "    \"\"\"\n",
    "    Processes one round of semi-supervised learning to generate pseudo-classes.\n",
    "\n",
    "    Parameters:\n",
    "    - X_lab: numpy array of labeled features\n",
    "    - X_unl: numpy array of unlabeled features\n",
    "    - y_lab: numpy array of labeled targets\n",
    "    - y_unl: numpy array of unlabeled targets\n",
    "    - model_i: model instance for semi-supervised learning\n",
    "    - base_model: base model instance for copying and setting hyperparameters\n",
    "    - random_state: int, seed for random number generation\n",
    "    - folds: int, number of folds for cross-validation\n",
    "    - threshold: float, threshold for pseudo-labeling\n",
    "    - optim_hp: bool, whether to optimize hyperparameters (default=False)\n",
    "    - verbose: bool, if True prints information and plots (default=False)\n",
    "\n",
    "    Returns:\n",
    "    - X_train: DataFrame of features for semi-supervised training\n",
    "    - y_train: Series of pseudo-labels for semi-supervised training\n",
    "    \"\"\"\n",
    "\n",
    "    # Creates a set of initial pseudo-labels following the Veenman method\n",
    "    df_combined = init_Veenman(X_lab, X_unl, y_lab, y_unl)\n",
    "    n_pseudo = 0 # to count how many pseudo-labels were used\n",
    "\n",
    "    # For the unlabeled classes, assign folders\n",
    "    df_combined = assign_folder(population = df_combined, folds = folds, random_state=random_state)\n",
    "    df_combined = df_combined.drop(columns=['labeled', 'label']).reset_index(drop=True) # removes real labels to avoid leakage to the unlabeled set.\n",
    "    \n",
    "    r_pseudo = 0 # to count how many pseudo-labels were used\n",
    "\n",
    "    # Initialize lists to keep track of indices to update\n",
    "    positive_indices_list = []\n",
    "\n",
    "    for fold in range(folds):\n",
    "        # Selects instances with labels (pseudo or true) to train a model. Cases in the current fold are not included.\n",
    "        condition = (df_combined['pseudo'] >= 0) & (df_combined['fold'] != fold)\n",
    "\n",
    "        # Removes identification columns\n",
    "        ss_train = df_combined[condition].drop(columns=['fold']).reset_index(drop=True)\n",
    "        X_train = ss_train.drop(columns=['pseudo']).reset_index(drop=True) # removes the labels from the set of features\n",
    "        y_train = ss_train['pseudo'].reset_index(drop=True) # selects only the features\n",
    "\n",
    "        # Instances in the test set will be checked. If they are predicted as 1, they will receive this pseudo-label.\n",
    "        X_test = df_combined[df_combined['fold'] == fold].drop(columns=['fold', 'pseudo']).reset_index(drop=True)\n",
    "        \n",
    "        if optim_hp:\n",
    "            if (fold == 0):\n",
    "                best_hp_SS_i = load_hp('Semi-supervised_i', population_proportion,\n",
    "                                    sample_bias, random_state)   \n",
    "                if best_hp_SS_i != False:\n",
    "                    model_i = copy.copy(base_model)\n",
    "                    model_i.steps[-1][1].set_params(**best_hp_SS_i)\n",
    "                    model_i.fit(X_train, y_train)\n",
    "                else:\n",
    "                    model_i.fit(X_train, y_train, callbacks=callback)\n",
    "                    best_hp_SS_i = model_i.best_params_\n",
    "                    best_hp_SS_i = {key.replace('randomforestclassifier__', ''): value for key, value in best_hp_SS_i.items()}\n",
    "                    save_hp('Semi-supervised_i', population_proportion, sample_bias, random_state, best_hp_SS_i)\n",
    "            else:\n",
    "                model_i = copy.copy(base_model)\n",
    "                model_i.steps[-1][1].set_params(**best_hp_SS_i)\n",
    "                model_i.fit(X_train, y_train)\n",
    "            print(\"Best Hyperparameters for the Random Forest:\", best_hp_SS_i)\n",
    "        else:\n",
    "            model_i = copy.copy(base_model)\n",
    "            model_i.fit(X_train, y_train)\n",
    "\n",
    "        # Gets the prediction probabilities for class 1\n",
    "        y_pred_prob = model_i.predict_proba(X_test)[:, 1]\n",
    "        # Selects the indices of the test set in the original full set.\n",
    "        test_indices = df_combined[df_combined['fold'] == fold].index\n",
    "\n",
    "        positive_indices = test_indices[y_pred_prob >= threshold]\n",
    "        if verbose:\n",
    "            print(\"Max prediction probability:\", y_pred_prob.max())\n",
    "            print(\"High probability of being 1:\", len(positive_indices))\n",
    "\n",
    "        # Store indices for updating after the inner loop\n",
    "        positive_indices_list.extend(positive_indices)\n",
    "\n",
    "        r_pseudo += len(positive_indices)\n",
    "\n",
    "    # Update the 'pseudo' and 'fold' columns in df_combined after the inner loop\n",
    "    df_combined.loc[positive_indices_list, 'pseudo'] = 1\n",
    "    df_combined.loc[positive_indices_list, 'fold'] = -1\n",
    "\n",
    "    n_pseudo += r_pseudo\n",
    "    if verbose:\n",
    "        print(r_pseudo, \"pseudo-labels inserted this round.\")\n",
    "\n",
    "    condition = (df_combined['pseudo'] >= 0) # only cases with true or pseudo-labels.\n",
    "    ss_final_train = df_combined[condition].drop(columns=['fold']).reset_index(drop=True)\n",
    "    X_train = ss_final_train.drop(columns=['pseudo']).reset_index(drop=True)\n",
    "    y_train = ss_final_train['pseudo'].reset_index(drop=True)\n",
    "\n",
    "    if verbose:\n",
    "        full_X = df_combined.drop(columns=['pseudo', 'fold']).reset_index(drop=True)\n",
    "        full_y = df_combined['pseudo'].reset_index(drop=True)\n",
    "        full_X.columns = full_X.columns.astype(str)\n",
    "        print(\"SAMPLE CREATED WITH SEMI-SUPERVISED METHOD\")\n",
    "        print(n_pseudo, \"pseudo-labels inserted.\")\n",
    "        describe_pop(X_train, y_train, full_X)\n",
    "        # PLOTS:\n",
    "        plot_population(full_X, full_y)\n",
    "\n",
    "    return(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Label Spreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LabSpred(X_lab, X_unl, y_lab, y_unl, X_test, y_test, opt_HP=0, verbose=False):\n",
    "    \"\"\"\n",
    "    Applies SciKit LabelSpreading\n",
    "\n",
    "    in:\n",
    "    - X_lab, X_unl, y_lab, y_unl\n",
    "    out:\n",
    "    - Dictionary with evaluation metrics (Precision, AUPRC, Precision@50, Precision@100, Bias effect)\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Concatenate labeled and unlabeled features (X_lab and X_unl) row-wise (rbind)\n",
    "    X = np.vstack((X_lab, X_unl))\n",
    "\n",
    "    # Step 2: Create a new array for unlabeled target values (y_unl) and set all values to -1\n",
    "    y_unl = np.full((X_unl.shape[0],), -1)\n",
    "\n",
    "    # Step 3: Concatenate labeled and unlabeled target values (y_lab and y_unl) row-wise (rbind)\n",
    "    y = np.concatenate((y_lab, y_unl))\n",
    "\n",
    "    if verbose:\n",
    "        describe_pop(X, y, full_X=X, plot_pop=True)\n",
    "\n",
    "    if opt_HP == 'no':\n",
    "        best_parameters = {'alpha': 0.1, 'gamma': 0.1, 'kernel': 'rbf', 'max_iter': 50, 'n_jobs': -1, 'tol': 0.0001}\n",
    "        print(\"Default hyperparameters will be used:\", best_parameters)\n",
    "    else:\n",
    "        best_parameters = opt_HP\n",
    "        print(\"Hyperparameters being used:\", best_parameters)\n",
    "\n",
    "    label_spr_model = LabelSpreading(**best_parameters)\n",
    "    label_spr_model.fit(X, y)\n",
    "    # Predict probabilities and labels on the test set\n",
    "    y_pred_prob = label_spr_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = label_spr_model.predict(X_test)\n",
    "\n",
    "    prec_score = precision_score(y_test, y_pred, zero_division=0)\n",
    "    y_pred_prob = np.nan_to_num(y_pred_prob, nan=0.0)\n",
    "    auprc_score = average_precision_score(y_test, y_pred_prob)\n",
    "    top_50 = top_k_precision_scorer(y_test, y_pred_prob, k = 50)\n",
    "    top_100 = top_k_precision_scorer(y_test, y_pred_prob, k = 100)\n",
    "    bias = measure_bias(X_test, y_test, y_pred_prob)[1]\n",
    "    if verbose:\n",
    "        print(\"Precision score:\", prec_score)\n",
    "        print(\"AUPRC score:\", auprc_score)\n",
    "        print(\"Precision on top 50:\", top_50)\n",
    "        print(\"Precision on top 100:\", top_100)\n",
    "        print(\"Prediction bias:\", bias) \n",
    "\n",
    "    result = {'pr': prec_score, 'AUPRC': auprc_score, 'p.50':top_50, 'p.100':top_100, 'Bias effect': bias}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Label Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LabProp(X_lab, X_unl, y_lab, y_unl, X_test, y_test, opt_HP=0, verbose=False):\n",
    "    \"\"\"\n",
    "    Applies SciKit LabelPropagation\n",
    "\n",
    "    in:\n",
    "    - X_lab, X_unl, y_lab, y_unl (numpy arrays or pandas dataframes)\n",
    "    - X_test, y_test (numpy arrays or pandas dataframes for testing)\n",
    "    - opt_HP: Boolean indicating whether to use optimized hyperparameters (default: False)\n",
    "    - verbose: Boolean indicating whether to print verbose output (default: False)\n",
    "\n",
    "    out:\n",
    "    - Dictionary with evaluation metrics (Precision, AUPRC, Precision@50, Precision@100, Bias effect)\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Concatenate labeled and unlabeled features (X_lab and X_unl) row-wise (rbind)\n",
    "    X = np.vstack((X_lab, X_unl))\n",
    "\n",
    "    # Step 2: Create a new array for unlabeled target values (y_unl) and set all values to -1\n",
    "    y_unl = np.full((X_unl.shape[0],), -1)\n",
    "\n",
    "    # Step 3: Concatenate labeled and unlabeled target values (y_lab and y_unl) row-wise (rbind)\n",
    "    y = np.concatenate((y_lab, y_unl))\n",
    "\n",
    "    if verbose:\n",
    "        describe_pop(X, y, full_X=X, plot_pop=True)\n",
    "\n",
    "    if opt_HP == 'no':\n",
    "        best_parameters = {'n_jobs': -1}\n",
    "    else:\n",
    "        best_parameters = opt_HP\n",
    "\n",
    "    label_prop_model = LabelPropagation(**best_parameters, n_jobs=-1)\n",
    "    label_prop_model.fit(X, y)\n",
    "    # Predict probabilities and labels on the test set\n",
    "    y_pred_prob = label_prop_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = label_prop_model.predict(X_test)\n",
    "\n",
    "    prec_score = precision_score(y_test, y_pred, zero_division=0)\n",
    "    y_pred_prob = np.nan_to_num(y_pred_prob, nan=0.0)\n",
    "    auprc_score = average_precision_score(y_test, y_pred_prob)\n",
    "    top_50 = top_k_precision_scorer(y_test, y_pred_prob, k = 50)\n",
    "    top_100 = top_k_precision_scorer(y_test, y_pred_prob, k = 100)\n",
    "    bias = measure_bias(X_test, y_test, y_pred_prob)[1]\n",
    "    if verbose:\n",
    "        print(\"Precision score:\", prec_score)\n",
    "        print(\"AUPRC score:\", auprc_score)\n",
    "        print(\"Precision on top 50:\", top_50)\n",
    "        print(\"Precision on top 100:\", top_100)\n",
    "        print(\"Prediction bias:\", bias) \n",
    "\n",
    "    result = {'pr': prec_score, 'AUPRC': auprc_score, 'p.50':top_50, 'p.100':top_100, 'Bias effect': bias}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Self Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelfLearn(X_lab, X_unl, y_lab, y_unl, X_test, y_test, trad_model, opt_HP=0, verbose=False):\n",
    "    \"\"\"\n",
    "    Applies SciKit SelfTrainingClassifier for semi-supervised learning\n",
    "\n",
    "    in:\n",
    "    - X_lab, X_unl, y_lab, y_unl (numpy arrays or pandas dataframes)\n",
    "    - X_test, y_test (numpy arrays or pandas dataframes for testing)\n",
    "    - trad_model: Base estimator for self-training\n",
    "    - opt_HP: Boolean indicating whether to use optimized hyperparameters (default: False)\n",
    "    - verbose: Boolean indicating whether to print verbose output (default: False)\n",
    "\n",
    "    out:\n",
    "    - Dictionary with evaluation metrics (Precision, AUPRC, Precision@50, Precision@100, Bias effect)\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Concatenate labeled and unlabeled features (X_lab and X_unl) row-wise (rbind)\n",
    "    X = np.vstack((X_lab, X_unl))\n",
    "\n",
    "    # Step 2: Create a new array for unlabeled target values (y_unl) and set all values to -1\n",
    "    y_unl = np.full((X_unl.shape[0],), -1)\n",
    "\n",
    "    # Step 3: Concatenate labeled and unlabeled target values (y_lab and y_unl) row-wise (rbind)\n",
    "    y = np.concatenate((y_lab, y_unl))\n",
    "\n",
    "    if verbose:\n",
    "        describe_pop(X, y, full_X=X, plot_pop=True)\n",
    "\n",
    "    if opt_HP == 'no':\n",
    "        best_parameters = {'criterion': 'k_best', 'k_best': 3}\n",
    "    else:\n",
    "        best_parameters = opt_HP\n",
    "\n",
    "    label_prop_model = SelfTrainingClassifier(**best_parameters, base_estimator=trad_model)\n",
    "    label_prop_model.fit(X, y)\n",
    "    # Predict probabilities and labels on the test set\n",
    "    y_pred_prob = label_prop_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = label_prop_model.predict(X_test)\n",
    "\n",
    "    prec_score = precision_score(y_test, y_pred, zero_division=0)\n",
    "    y_pred_prob = np.nan_to_num(y_pred_prob, nan=0.0)\n",
    "    auprc_score = average_precision_score(y_test, y_pred_prob)\n",
    "    top_50 = top_k_precision_scorer(y_test, y_pred_prob, k = 50)\n",
    "    top_100 = top_k_precision_scorer(y_test, y_pred_prob, k = 100)\n",
    "    bias = measure_bias(X_test, y_test, y_pred_prob)[1]\n",
    "    if verbose:\n",
    "        print(\"Precision score:\", prec_score)\n",
    "        print(\"AUPRC score:\", auprc_score)\n",
    "        print(\"Precision on top 50:\", top_50)\n",
    "        print(\"Precision on top 100:\", top_100)\n",
    "        print(\"Prediction bias:\", bias) \n",
    "\n",
    "    result = {'pr': prec_score, 'AUPRC': auprc_score, 'p.50':top_50, 'p.100':top_100, 'Bias effect': bias}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results\n",
    "\n",
    "## 4.1 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hp(method, population_proportion, sample_bias, seed, hyperparameters, folder='hp_cache'):\n",
    "    \"\"\"\n",
    "    Save hyperparameters to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - method: Method identifier\n",
    "    - population_proportion: Population proportion parameter\n",
    "    - sample_bias: Sample bias parameter\n",
    "    - seed: Random seed\n",
    "    - hyperparameters: Dictionary of hyperparameters to save\n",
    "    - folder: Folder to save the JSON file (default: 'hp_cache')\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    filename = f\"{folder}/hp_{method}_{population_proportion}_{sample_bias}_{seed}.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(hyperparameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hp(method, population_proportion, sample_bias, seed, folder='hp_cache'):\n",
    "    \"\"\"\n",
    "    Load hyperparameters from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - method: Method identifier\n",
    "    - population_proportion: Population proportion parameter\n",
    "    - sample_bias: Sample bias parameter\n",
    "    - seed: Random seed\n",
    "    - folder: Folder containing the JSON file (default: 'hp_cache')\n",
    "\n",
    "    Returns:\n",
    "    - hyperparameters: Loaded hyperparameters as a dictionary, or False if file not found\n",
    "    \"\"\"\n",
    "    filename = f\"{folder}/hp_{method}_{population_proportion}_{sample_bias}_{seed}.json\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(folder='scenario_results', output_excel='aggregated_results.xlsx', output_csv='aggregated_results.csv'):\n",
    "    \"\"\"\n",
    "    Save aggregated scenario results to Excel and CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - folder: Folder containing scenario results (default: 'scenario_results')\n",
    "    - output_excel: Output Excel file name (default: 'aggregated_results.xlsx')\n",
    "    - output_csv: Output CSV file name (default: 'aggregated_results.csv')\n",
    "\n",
    "    Returns:\n",
    "    - df: Aggregated results DataFrame\n",
    "    \"\"\"\n",
    "    # Load all scenario results from the folder\n",
    "    scenario_results = load_all_scenario_results(folder)\n",
    "    \n",
    "    # Initialize an empty list to aggregate all results\n",
    "    aggregated_results = []\n",
    "    \n",
    "    # Iterate through each scenario result\n",
    "    for result in scenario_results:\n",
    "        aggregated_results.extend(result)\n",
    "    \n",
    "    # Convert to DataFrame for easy manipulation (optional)\n",
    "    df = pd.DataFrame(scenario_results)\n",
    "    \n",
    "    # Save aggregated results to Excel\n",
    "    df.to_excel(output_excel, index=False)\n",
    "\n",
    "    # Save aggregated results to CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    # Optionally, return the aggregated results DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scenario_result(scenario_result, method, population_proportion, sample_bias, seed, folder='scenario_results'):\n",
    "    \"\"\"\n",
    "    Save a specific scenario result to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - scenario_result: Dictionary containing the scenario result\n",
    "    - method: Method identifier\n",
    "    - population_proportion: Population proportion parameter\n",
    "    - sample_bias: Sample bias parameter\n",
    "    - seed: Random seed\n",
    "    - folder: Folder to save the JSON file (default: 'scenario_results')\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    filename = f\"{folder}/result_{method}_{population_proportion}_{sample_bias}_{seed}.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(scenario_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_scenario_results(folder='scenario_results'):\n",
    "    \"\"\"\n",
    "    Load all scenario results from JSON files in a folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder: Folder containing scenario result JSON files (default: 'scenario_results')\n",
    "\n",
    "    Returns:\n",
    "    - results: List of loaded scenario results (each element is a dictionary)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    if os.path.exists(folder):\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith('.json'):\n",
    "                with open(os.path.join(folder, filename), 'r') as f:\n",
    "                    results.append(json.load(f))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_methods(seeds,\n",
    "        population_proportions=[0.05, 0.5],\n",
    "        sample_biases=[0.95, 0.5, 0],\n",
    "        methods=['traditional', 'Veenman', 'Semi-supervised', 'Label_spreading', 'Label_Propagation', 'Self_Learning'],\n",
    "        verbose=True):\n",
    "    \n",
    "    results = []  # Initialize the results list\n",
    "\n",
    "    for seed in seeds:\n",
    "        np.random.seed(seed)\n",
    "        trad_model = []\n",
    "\n",
    "        base_model, model, cv_stratified = initialize_models(seed)\n",
    "        \n",
    "        # Iterate through population proportions\n",
    "        for population_proportion in population_proportions:\n",
    "            # Create population with the given proportion\n",
    "            X_cur_pop, X_fut_pop, y_cur_pop, y_fut_pop = create_pop(random_state = seed, ratio=population_proportion)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"===============================================================================\")\n",
    "                print(f\"New population created with proportion={population_proportion}\")\n",
    "                print(\"Current Population:\")\n",
    "                describe_pop(X_cur_pop, y_cur_pop, full_X=X_cur_pop, plot_pop=True)\n",
    "                print()\n",
    "                print(\"Future Population:\")\n",
    "                describe_pop(X_fut_pop, y_fut_pop, full_X=X_cur_pop, plot_pop=True)\n",
    "                print()\n",
    "            \n",
    "            # Iterate through sample biases\n",
    "            for sample_bias in sample_biases:\n",
    "\n",
    "                # Create labeled and unlabeled instances with the given bias and proportion\n",
    "                X_lab, X_unl, y_lab, y_unl = label_cases(X=X_cur_pop, y=y_cur_pop,\n",
    "                                                         random_state = seed,\n",
    "                                                         sample_bias=sample_bias,\n",
    "                                                         verbose=verbose\n",
    "                                                        )\n",
    "\n",
    "                # Saves information about how biased the sample is in relation to the population, for class 1\n",
    "                original_bias = measure_bias_in_sample(X_cur_pop, X_lab, y_cur_pop, y_lab, verbose=verbose)\n",
    "                if verbose:\n",
    "                    print(\"Distance between the centroid of the sample and the population for class 1:\", original_bias[1])\n",
    "                result_temp = {\n",
    "                            \"Seed\": seed,\n",
    "                            \"Prop. class 1\": population_proportion,\n",
    "                            \"Sel. bias\": sample_bias,\n",
    "                            \"Method\": \"sample\",\n",
    "                            \"Test set\": np.NaN,\n",
    "                            \"pr\": np.NaN,\n",
    "                            \"AUPRC\": np.NaN,\n",
    "                            \"p.50\": np.NaN,\n",
    "                            \"p.100\": np.NaN,\n",
    "                            \"Bias effect\": original_bias[1]\n",
    "                        }\n",
    "                save_scenario_result(result_temp, \"sample\", population_proportion, sample_bias, seed)\n",
    "\n",
    "                \n",
    "                for method in methods:\n",
    "                    if method == 'traditional':\n",
    "                        if verbose:\n",
    "                            print(\"===============================================================================\")\n",
    "                            print(\"TRADITIONAL METHOD APPLIED\")\n",
    "                            print(\"\")\n",
    "                            print(\"Performance on future population:\")\n",
    "                        \n",
    "                        # Test performance on future population\n",
    "                        result_temp = {\n",
    "                            \"Seed\": seed,\n",
    "                            \"Prop. class 1\": population_proportion,\n",
    "                            \"Sel. bias\": sample_bias,\n",
    "                            \"Method\": method,\n",
    "                            \"Test set\": \"future\"\n",
    "                        }\n",
    "                        trad_model = copy.copy(model)\n",
    "                        if optim_hp:\n",
    "                            best_hp_t = load_hp(method, population_proportion, sample_bias, seed)     \n",
    "                            if best_hp_t != False:\n",
    "                                trad_model = copy.copy(base_model)\n",
    "                                trad_model.steps[-1][1].set_params(**best_hp_t)\n",
    "                            \n",
    "                        Trad_temp = test_performance(X_train=X_lab, y_train=y_lab,\n",
    "                                                    X_test=X_fut_pop, y_test=y_fut_pop,\n",
    "                                                    model_i=trad_model,\n",
    "                                                    std_model=model,\n",
    "                                                    verbose=verbose)\n",
    "                        if optim_hp and (best_hp_t is False):\n",
    "                            best_hp_t = Trad_temp[1]\n",
    "                            save_hp(method, population_proportion, sample_bias, seed, best_hp_t)\n",
    "                            trad_model = copy.copy(base_model)\n",
    "                            trad_model.steps[-1][1].set_params(**best_hp_t)\n",
    "\n",
    "                        result_temp.update(Trad_temp[0])\n",
    "                        results.append(result_temp)\n",
    "                        save_scenario_result(result_temp, method, population_proportion, sample_bias, seed)\n",
    "                        \n",
    "                    elif method == 'Veenman':\n",
    "                        if verbose:\n",
    "                            print(\"===============================================================================\")\n",
    "                            print(\"VEENMAN METHOD APPLIED\")\n",
    "                            print()\n",
    "                            print(\"Performance on future population:\")\n",
    "\n",
    "                        # Apply the Veenman method and test performance\n",
    "                        X_V_lab, y_V_lab = Veenman(X_lab, X_unl, y_lab, y_unl, verbose=verbose)\n",
    "                        # Performance on future population\n",
    "                        result_temp = {\n",
    "                            \"Seed\": seed,\n",
    "                            \"Prop. class 1\": population_proportion,\n",
    "                            \"Sel. bias\": sample_bias,\n",
    "                            \"Method\": method,\n",
    "                            \"Test set\": \"future\"\n",
    "                        }\n",
    "                        veenman_model = copy.copy(model)\n",
    "                        if optim_hp:\n",
    "                            best_hp_V = load_hp(method, population_proportion, sample_bias, seed)     \n",
    "                            if best_hp_V != False:\n",
    "                                veenman_model = copy.copy(base_model)\n",
    "                                veenman_model.steps[-1][1].set_params(**best_hp_V)\n",
    "\n",
    "                        Veenman_temp = test_performance(X_train=X_V_lab, y_train=y_V_lab,\n",
    "                                                        X_test=X_fut_pop, y_test=y_fut_pop,\n",
    "                                                        model_i=veenman_model,\n",
    "                                                        std_model=model,\n",
    "                                                        verbose=verbose)\n",
    "                        if optim_hp and (best_hp_V is False):\n",
    "                            best_hp_V = Veenman_temp[1]\n",
    "                            save_hp(method, population_proportion, sample_bias, seed, best_hp_V)\n",
    "\n",
    "                        result_temp.update(Veenman_temp[0])\n",
    "                        results.append(result_temp)\n",
    "                        save_scenario_result(result_temp, method, population_proportion, sample_bias, seed)\n",
    "                        \n",
    "                    elif method == 'Semi-supervised':\n",
    "                        if verbose:\n",
    "                            print(\"===============================================================================\")\n",
    "                            print(\"SEMI-SUPERVISED METHOD APPLIED\")\n",
    "                            print()\n",
    "                            print(\"Performance on future population:\")\n",
    "\n",
    "                        # Performance on future population\n",
    "                        result_temp = {\n",
    "                            \"Seed\": seed,\n",
    "                            \"Prop. class 1\": population_proportion,\n",
    "                            \"Sel. bias\": sample_bias,\n",
    "                            \"Method\": method,\n",
    "                            \"Test set\": \"future\"\n",
    "                        }\n",
    "\n",
    "                        if optim_hp:\n",
    "                            ss_model = copy.copy(model)\n",
    "                        else:\n",
    "                            ss_model = copy.copy(base_model)\n",
    "\n",
    "                        X_ss_lab, y_ss_lab = semi_supervised(X_lab, X_unl, y_lab, y_unl,\n",
    "                                                            model_i=ss_model,\n",
    "                                                            random_state=seed,\n",
    "                                                            base_model=base_model,\n",
    "                                                            verbose=verbose,\n",
    "                                                            population_proportion=population_proportion,\n",
    "                                                            sample_bias=sample_bias)\n",
    "                        \n",
    "                        if optim_hp:\n",
    "                            best_hp_SS = load_hp(method, population_proportion, sample_bias, seed)     \n",
    "                            if best_hp_SS != False:\n",
    "                                ss_model = copy.copy(base_model)\n",
    "                                ss_model.steps[-1][1].set_params(**best_hp_SS)\n",
    "\n",
    "                        ss_temp = test_performance(X_train=X_ss_lab, y_train=y_ss_lab,\n",
    "                                                            X_test=X_fut_pop, y_test=y_fut_pop,\n",
    "                                                            model_i=ss_model,\n",
    "                                                            std_model=model,\n",
    "                                                            verbose=verbose)\n",
    "                        \n",
    "                        if optim_hp and (best_hp_SS is False):\n",
    "                            best_hp_SS = ss_temp[1]\n",
    "                            save_hp(method, population_proportion, sample_bias, seed, best_hp_SS)\n",
    "\n",
    "                        result_temp.update(ss_temp[0])\n",
    "                        results.append(result_temp)\n",
    "                        save_scenario_result(result_temp, method, population_proportion, sample_bias, seed)\n",
    "                        \n",
    "                    elif method == 'Label_spreading':\n",
    "                        if verbose:\n",
    "                            print(\"===============================================================================\")\n",
    "                            print(\"LABEL SPREADING\")\n",
    "                            print()\n",
    "                            print(\"Performance on future population:\")\n",
    "\n",
    "                        # Performance on future population\n",
    "                        result_temp = {\n",
    "                            \"Seed\": seed,\n",
    "                            \"Prop. class 1\": population_proportion,\n",
    "                            \"Sel. bias\": sample_bias,\n",
    "                            \"Method\": method,\n",
    "                            \"Test set\": \"future\"\n",
    "                        }\n",
    "\n",
    "                        if optim_hp:\n",
    "                            best_hp_LS = load_hp(method, population_proportion, sample_bias, seed)\n",
    "                            if best_hp_LS is False:\n",
    "                                best_hp_LS = optim_par(X = np.vstack((X_lab, X_unl)),\n",
    "                                                       y = np.concatenate((y_lab, np.full((X_unl.shape[0],), -1))),\n",
    "                                                       random_state=seed, cv_stratified=cv_stratified,\n",
    "                                                       method='label_spreading')\n",
    "                                save_hp(method, population_proportion, sample_bias, seed, best_hp_LS)\n",
    "                                if verbose:\n",
    "                                    print(\"Optimized hyperparameters: \", best_hp_LS)\n",
    "                        else:\n",
    "                            best_hp_LS = 'no'\n",
    "\n",
    "                        result_temp.update(LabSpred(X_lab, X_unl, y_lab, y_unl,\n",
    "                                                    X_test=X_fut_pop,\n",
    "                                                    y_test=y_fut_pop,\n",
    "                                                    opt_HP=best_hp_LS,\n",
    "                                                    verbose=verbose)\n",
    "                        )                    \n",
    "                        results.append(result_temp)\n",
    "                        save_scenario_result(result_temp, method, population_proportion, sample_bias, seed)\n",
    "\n",
    "                    elif method == 'Label_Propagation':\n",
    "                        if verbose:\n",
    "                            print(\"===============================================================================\")\n",
    "                            print(\"LABEL PROPAGATION\")\n",
    "                            print()\n",
    "                            print(\"Performance on future population:\")\n",
    "\n",
    "                        # Performance on future population\n",
    "                        result_temp = {\n",
    "                            \"Seed\": seed,\n",
    "                            \"Prop. class 1\": population_proportion,\n",
    "                            \"Sel. bias\": sample_bias,\n",
    "                            \"Method\": method,\n",
    "                            \"Test set\": \"future\"\n",
    "                        }\n",
    "\n",
    "                        if optim_hp:\n",
    "                            best_hp_LP = load_hp(method, population_proportion, sample_bias, seed)\n",
    "                            if best_hp_LP is False:\n",
    "                                best_hp_LP = optim_par(X = np.vstack((X_lab, X_unl)),\n",
    "                                                       y = np.concatenate((y_lab, np.full((X_unl.shape[0],), -1))),\n",
    "                                                       random_state=seed, cv_stratified=cv_stratified,\n",
    "                                                       method='lab_propag')\n",
    "                                save_hp(method, population_proportion, sample_bias, seed, best_hp_LP)\n",
    "                                if verbose:\n",
    "                                    print(\"Optimized hyperparameters: \", best_hp_LP)\n",
    "                        else:\n",
    "                            best_hp_LP = 'no'\n",
    "\n",
    "                        result_temp.update(LabProp(X_lab, X_unl, y_lab, y_unl,\n",
    "                                                X_test=X_fut_pop,\n",
    "                                                y_test=y_fut_pop,\n",
    "                                                opt_HP=best_hp_LP,\n",
    "                                                verbose=verbose)\n",
    "                        )                    \n",
    "                        results.append(result_temp)\n",
    "                        save_scenario_result(result_temp, method, population_proportion, sample_bias, seed)\n",
    "\n",
    "                    elif method == 'Self_Learning':\n",
    "                        if verbose:\n",
    "                            print(\"===============================================================================\")\n",
    "                            print(\"SELF LEARNING\")\n",
    "                            print()\n",
    "                            print(\"Performance on future population:\")\n",
    "\n",
    "                        # Performance on future population\n",
    "                        result_temp = {\n",
    "                            \"Seed\": seed,\n",
    "                            \"Prop. class 1\": population_proportion,\n",
    "                            \"Sel. bias\": sample_bias,\n",
    "                            \"Method\": method,\n",
    "                            \"Test set\": \"future\"\n",
    "                        }\n",
    "\n",
    "                        if optim_hp:\n",
    "                            best_hp_SL = load_hp(method, population_proportion, sample_bias, seed)\n",
    "                            if best_hp_SL is False:\n",
    "                                best_hp_SL = optim_par(X = np.vstack((X_lab, X_unl)),\n",
    "                                                       y = np.concatenate((y_lab, np.full((X_unl.shape[0],), -1))),\n",
    "                                                       random_state=seed, cv_stratified=cv_stratified,\n",
    "                                                       method='self-learning', base_model = trad_model)\n",
    "                                save_hp(method, population_proportion, sample_bias, seed, best_hp_SL)\n",
    "                                if verbose:\n",
    "                                    print(\"Optimized hyperparameters: \", best_hp_SL)\n",
    "                        else:\n",
    "                            best_hp_SL = 'no'\n",
    "                        \n",
    "                        result_temp.update(SelfLearn(X_lab, X_unl, y_lab, y_unl,\n",
    "                                                    X_test=X_fut_pop,\n",
    "                                                    y_test=y_fut_pop,\n",
    "                                                    trad_model=trad_model,\n",
    "                                                    opt_HP=best_hp_SL,\n",
    "                                                    verbose=verbose)\n",
    "                        )                    \n",
    "                        results.append(result_temp)\n",
    "                        save_scenario_result(result_temp, method, population_proportion, sample_bias, seed)\n",
    "                        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Run Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run!\n",
    "seeds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44] # Hp optimized.\n",
    "#skipped: seeds 11, 28, 37. Don't generate consistent samples with at least one observation of class 1.\n",
    "results = run_methods(seeds=seeds, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Save Aggregated Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(folder='scenario_results', output_excel='aggregated_results.xlsx', output_csv='aggregated_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Visualise Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.to_excel(\"results.xlsx\", index=False)\n",
    "\n",
    "# Define the styling function\n",
    "def style_rows(row):\n",
    "    # Set font color to black and alternate background colors every four rows\n",
    "    font_color = 'color: black;'\n",
    "    row_color = 'background-color: '\n",
    "    if row.name % 4 == 0:\n",
    "        return [font_color + row_color + 'lightgray'] * len(row)\n",
    "    elif row.name % 4 == 1:\n",
    "        return [font_color + row_color + 'white'] * len(row)\n",
    "    elif row.name % 4 == 2:\n",
    "        return [font_color + row_color + 'lightblue'] * len(row)\n",
    "    else:\n",
    "        return [font_color + row_color + 'lightgreen'] * len(row)\n",
    "\n",
    "\n",
    "# Apply the styling function\n",
    "styled_df = results_df.style.apply(style_rows, axis=1)\n",
    "\n",
    "# Display the styled DataFrame as an HTML table\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hyperparameter Used During the Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hyperparameters(folder='hp_cache'):\n",
    "    # Dictionary to store hyperparameters by method\n",
    "    hyperparams_by_method = {\n",
    "        'traditional': [],\n",
    "        'Veenman': [],\n",
    "        'Semi-supervised': [],\n",
    "        'Semi-supervised_i': [],\n",
    "        'Label_spreading': [],\n",
    "        'Label_Propagation': [],\n",
    "        'Self_Learning': []\n",
    "    }\n",
    "\n",
    "    # Read all JSON files in the folder\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            with open(filepath, 'r') as f:\n",
    "                hyperparameters = json.load(f)\n",
    "                \n",
    "            # Extract the method from the filename\n",
    "            method_parts = filename.split('_')[1:-3]  # Extract parts between 'hp' and the last three elements\n",
    "            method = '_'.join(method_parts)\n",
    "            \n",
    "            # Group into the appropriate category\n",
    "            if method in hyperparams_by_method:\n",
    "                hyperparams_by_method[method].append(hyperparameters)\n",
    "            else:\n",
    "                print(f\"Unknown method {method} in file {filename}\")\n",
    "\n",
    "    return hyperparams_by_method\n",
    "\n",
    "def create_hyperparameter_tables(hyperparams_by_method):\n",
    "    # Create a dictionary to store the DataFrames for each table\n",
    "    tables = {}\n",
    "\n",
    "    # Combine Traditional, Veenman, Semi-supervised, and Semi-supervised_i into one table\n",
    "    combined_methods = ['traditional', 'Veenman', 'Semi-supervised', 'Semi-supervised_i']\n",
    "    combined_hyperparams = []\n",
    "    for method in combined_methods:\n",
    "        combined_hyperparams.extend(hyperparams_by_method[method])\n",
    "    \n",
    "    if combined_hyperparams:\n",
    "        combined_df = pd.DataFrame(combined_hyperparams)\n",
    "        tables['Combined'] = combined_df\n",
    "\n",
    "    # Create separate tables for other methods\n",
    "    other_methods = ['Label_spreading', 'Label_Propagation', 'Self_Learning']\n",
    "    for method in other_methods:\n",
    "        if hyperparams_by_method[method]:\n",
    "            df = pd.DataFrame(hyperparams_by_method[method])\n",
    "            tables[method] = df\n",
    "    \n",
    "    return tables\n",
    "\n",
    "def display_tables(tables):\n",
    "    for method, table in tables.items():\n",
    "        print(f\"\\nHyperparameters for {method} method:\")\n",
    "        display(table)\n",
    "\n",
    "# Read hyperparameters from folder\n",
    "hyperparams_by_method = read_hyperparameters()\n",
    "\n",
    "# Create hyperparameter tables\n",
    "tables = create_hyperparameter_tables(hyperparams_by_method)\n",
    "\n",
    "# Display the tables\n",
    "display_tables(tables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
